{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Using LLM's\n",
    "### How this tool works: -\n",
    "1. Sending comments in batches of 2000 to gpt-4's 128k model to get initial topics which the comments are divided into  \n",
    "2. The summaries for these batches are then consolidated into one JSON file with details that include comment themes, percentage of the theme present in the data, number of positive and negative comments for each topic\n",
    "3. Save the generated JSON file which we can use for future comparisons\n",
    "4. We then pass the comments again, along with the themes discovered in the first pass, to get sub-themes of each theme.\n",
    "5. We save the sub-themes in a separate JSON for future reference.\n",
    "6. Using the data generated we plot 3 graphs; 1. where the percentage of themes is plotted. 2. The number of positive and negative comments for each topic. 3. Sub-topics for each Topic\n",
    "7. Finally, we can ask specific question to the data using PandasAI (Eg. How many customers in the survey speak French?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Azure LLM\n",
    "We use gpt-4-1106-preview with 128k tokens as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import csv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "#AzureOpenAI setup\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    # api_type='azure',\n",
    "    api_version='2023-05-15',  # this may change in the future\n",
    "    timeout=20*60,  # 20 minutes\n",
    ")\n",
    "\n",
    "#To generate summaries from 1000's of comments\n",
    "def generate_summary(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        temperature=0.0,\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\":\"\"\"You are a product manager.\n",
    "                \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": prompt },\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "#To consolidate different Summaries (The prompt \"Business Analyst\" makes summaries more concise and to the point.)\n",
    "def consolidate_summaries(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "    temperature=0.0,\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\":\"\"\"You are a Business Analyst.\n",
    "            \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": prompt },\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from the csv\n",
    "We can change the name of the CSV and the column name that we want to analyse over here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = \"Q2 - Comment - Recommendations\"  #We can extract the column we want to analyse by providing the appropriate name.\n",
    "with open(\"survey.csv\", 'r', newline='') as csvfile: #We can the input file here.\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    if column_name not in reader.fieldnames:\n",
    "        print(\"Field name not found!\")\n",
    "    column_data = [row[column_name] for row in reader if row[column_name].strip()]\n",
    "\n",
    "print(f\"Number of comments in the column: {len(column_data)}\")\n",
    "\n",
    "#Making batches of 2000 comments\n",
    "batch_size = 2000 #We can change the number of comments in each batch, depending on the survey type.\n",
    "total_batches = (len(column_data) + batch_size - 1)//batch_size\n",
    "print(f\"Number of batches: {total_batches}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing comments in batches to establish Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_responses = []\n",
    "\n",
    "for batch_number in range(total_batches):\n",
    "    start_index = batch_number * batch_size\n",
    "    end_index = (batch_number + 1) * batch_size\n",
    "    comment_batch = column_data[start_index:end_index]\n",
    "    prompt = \"\"\"As a poduct manager, analyze if the comments seperated by a single quotes are positive or negative and give the weight by occurence of each topic % \n",
    "    give a summary in the following JSON format.\n",
    "        {\n",
    "    \"date\": \"Todays_Date\",\n",
    "    \"theme_weights\": {\n",
    "        \"Theme 1\": [{theme1_percentage}%, number of positive comments, number of negative comments],\",\n",
    "        \"Theme 2\": [{theme2_percentage}%, number of positive comments, number of negative comments]%\",\n",
    "        \"Theme 3\": [{theme3_percentage}%, number of positive comments, number of negative comments]%\n",
    "    }\n",
    "    \"Summary\": \"Consolidated Summary of all the comments\"\n",
    "    }\n",
    "    comments:\"\"\" + f\"```{comment_batch}```\"\n",
    "    summary = generate_summary(prompt)\n",
    "    print(f\"######Summary of batch number {batch_number}: -\")\n",
    "    print(summary)\n",
    "    appended_responses.append(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidating different responses recieved from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = \"\"\"\n",
    "As a business Analyst, \n",
    "analyze the following summaries seperated by single quotes and give the consolidated weight by occurence percentage(%), number of positive comments and number of negative commenst for every theme along with a comprehensive summary,\n",
    "Generate a JSON file in the following format:\n",
    "\n",
    "{\n",
    "  \"date\": \"Todays_Date\",\n",
    "  \"theme_weights\": {\n",
    "      \"Theme 1\": [{theme1_percentage}%, {number of positive comments}, {number of negative comments}],\",\n",
    "      \"Theme 2\": [{theme2_percentage}%, {number of positive comments}, {number of negative comments}]\",\n",
    "      \"Theme 3\": [{theme3_percentage}%, {number of positive comments}, {number of negative comments}]\n",
    "  }\n",
    "  \"Summary\": \"Consolidated Summary of all the comments\"\n",
    "}\n",
    "\"\"\" + f\"Seperate Summaries:```{appended_responses}```\"\n",
    "\n",
    "#Comparing the summaries and generating a final summary\n",
    "final_analysis = consolidate_summaries(prompt3)\n",
    "print(final_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the JSON file containing Main Topics with +ve & -ve comments \n",
    "It is saved as \"analysis_<todays_date>.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import date\n",
    "\n",
    "start_index = final_analysis.find('{')\n",
    "end_index = final_analysis.rfind('}')\n",
    "\n",
    "if start_index != -1 and end_index != -1:\n",
    "    json_part = final_analysis[start_index:end_index+1]\n",
    "    print(json_part)\n",
    "    try:\n",
    "        # Load the JSON part using a JSON parser\n",
    "        json_data = json.loads(json_part)\n",
    "        json_data[\"date\"] = str(date.today()) #GPT can't get dates right :(\n",
    "        print(\"Extracted JSON:\", json_data)\n",
    "        file_path = f\"analysis_{str(date.today())}.json\"\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(json_data, json_file, indent=4)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Invalid JSON structure.\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Pass to find subtopics under Initial themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_responses = []\n",
    "\n",
    "for batch_number in range(total_batches):\n",
    "    start_index = batch_number * batch_size\n",
    "    end_index = (batch_number + 1) * batch_size\n",
    "    comment_batch = column_data[start_index:end_index]\n",
    "    prompt = \"\"\"As a poduct manager, analyze the comments and subdivide them into categories along with weighted percentage of occurance, based on the initial analysis. \n",
    "    give a summary in the following JSON format.\n",
    "        {\n",
    "    \"date\": \"Todays_Date\",\n",
    "    \"theme_weights\": {\n",
    "        \"Theme 1\": [category 1, percentage occurance of category 1, category 2 percentage occurance of category 2, category 3, percentage occurance of category 3],\",\n",
    "        \"Theme 2\": [category 1, percentage occurance of category 1, category 2, percentage occurance of category 2, category 3, percentage occurance of category 3]\",\n",
    "        \"Theme 3\": [category 1, percentage occurance of category 1, category 2, percentage occurance of category 2, category 3, percentage occurance of category 3]\n",
    "    }\n",
    "    \"Summary\": \"Consolidated Summary of all the comments\"\n",
    "    }\n",
    "    comments:\"\"\" + f\"```{comment_batch}```\" + f\"Previous Summary: {json_data}\"\n",
    "    summary = generate_summary(prompt)\n",
    "    print(f\"######Summary of batch number {batch_number}: -\")\n",
    "    print(summary)\n",
    "    appended_responses.append(summary)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt4 = \"\"\"\n",
    "As a business Analyst, \n",
    "analyze the following summaries seperated by triple backticks and give the complete list of subtopics for each them. Include a comprehensive summary,\n",
    "Generate a JSON file in the following format:\n",
    "\n",
    "{\n",
    "  \"date\": \"Todays_Date\",\n",
    "  \"theme_weights\": {\n",
    "        \"Theme 1\": [category 1, percentage occurance of category 1, category 2 percentage occurance of category 2, category 3, percentage occurance of category 3],\",\n",
    "        \"Theme 2\": [category 1, percentage occurance of category 1, category 2, percentage occurance of category 2, category 3, percentage occurance of category 3]\",\n",
    "        \"Theme 3\": [category 1, percentage occurance of category 1, category 2, percentage occurance of category 2, category 3, percentage occurance of category 3]\n",
    "  }\n",
    "  \"Summary\": \"Consolidated Summary of all the comments\"\n",
    "}\n",
    "\"\"\" + f\"Seperate Summaries:```{appended_responses}```\"\n",
    "\n",
    "#Comparing the summaries and generating a final summary\n",
    "final_themes = consolidate_summaries(prompt4)\n",
    "print(final_themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = final_themes.find('{')\n",
    "end_index = final_themes.rfind('}')\n",
    "\n",
    "if start_index != -1 and end_index != -1:\n",
    "    json_part = final_themes[start_index:end_index+1]\n",
    "    print(json_part)\n",
    "    try:\n",
    "        # Load the JSON part using a JSON parser\n",
    "        theme_json_data = json.loads(json_part)\n",
    "        theme_json_data[\"date\"] = str(date.today()) #GPT can't get dates right :(\n",
    "        print(\"Extracted JSON:\", theme_json_data)\n",
    "        file_path = f\"subtopic_analysis_{str(date.today())}.json\"\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(json_data, json_file, indent=4)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Invalid JSON structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the data for percentage of occurance of themes \n",
    "Here we can see the most common topics that are mentioned in the survey along with an overall summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6)) #Setting the figure size in inches\n",
    "\n",
    "# Extracting data for plotting\n",
    "date = json_data[\"date\"]\n",
    "theme_weights = json_data[\"theme_weights\"]\n",
    "\n",
    "themes = list(theme_weights.keys())\n",
    "percentages = [float(weight[0][:-1]) for weight in theme_weights.values()]\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.bar(themes, percentages, color='blue')\n",
    "plt.xticks(rotation=45, ha='right') #Rotating and aligning the x-axis labels\n",
    "plt.xlabel('Comment Themes')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title(f'Most Common Comments on {date}')\n",
    "plt.tight_layout()  # Adjusts the layout\n",
    "plt.savefig('bar_plot.png', dpi=300)  # saves the plot as 'bar_plot.png' with the specified dpi\n",
    "plt.show()\n",
    "\n",
    "print(f\"Executive Summary: {json_data['Summary']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Positive and Negative comments for each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extracting data for plotting\n",
    "themes = list(theme_weights.keys())\n",
    "positive_comments = [weight[1] for weight in theme_weights.values()]\n",
    "negative_comments = [-weight[2] for weight in theme_weights.values()]\n",
    "\n",
    "pos = np.arange(len(themes))\n",
    "width = 0.4\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(pos - width/2, positive_comments, width, color='green', label='Positive Comments')\n",
    "plt.bar(pos + width/2, negative_comments, width, color='red', label='Negative Comments')\n",
    "plt.xticks(pos, themes, rotation=45, ha='right')\n",
    "plt.xlabel('Themes')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Theme Weights')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()  # Adjusts the layout\n",
    "plt.savefig('comment_distribution.png', dpi=300)  # saves the plot as 'theme_weights.png' with a resolution of 300 dpi\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Weights of Sub-Topics for Each Major Theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the data into a suitable format\n",
    "subtopic_name = 'Call Quality'  #We can change the subtopic name here to get the subtopics for that theme.\n",
    "\n",
    "# Extracting the subtopic data for a specific theme\n",
    "subtopic_data = theme_json_data['theme_weights'][subtopic_name] \n",
    "\n",
    "categories = {}\n",
    "for i in range(0, len(subtopic_data), 2):\n",
    "    category = subtopic_data[i]\n",
    "    value = subtopic_data[i+1]\n",
    "    if isinstance(value, str) and '%' in value:  # Check if value is a percentage\n",
    "        value = float(value.replace('%', ''))  # Remove '%' and convert to float\n",
    "    else:\n",
    "        value = float(value)  # Convert to float\n",
    "    categories[category] = value\n",
    "\n",
    "labels = list(categories.keys())\n",
    "values = list(categories.values())\n",
    "\n",
    "plt.barh(labels, values, color='blue')\n",
    "plt.xlabel('Values')\n",
    "plt.title(subtopic_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with previous data stored as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing json_data with the previous data\n",
    "\n",
    "#Reading data from JSON file\n",
    "with open('analysis_2023-04-14.json') as json_file: #We can change the file name here to the file we wish to compare the data with.\n",
    "    old_data = json.load(json_file)\n",
    "\n",
    "#Extracting data for plotting\n",
    "old_date = old_data[\"date\"]\n",
    "old_theme_weights = old_data[\"theme_weights\"]\n",
    "\n",
    "old_themes = list(old_theme_weights.keys())\n",
    "old_percentages = [float(weight[0][:-1]) for weight in old_theme_weights.values()]\n",
    "\n",
    "#comparison of the data\n",
    "plt.figure(figsize=(10, 6)) #Setting the figure size in inches\n",
    "plt.plot(themes, percentages,label='Data of current Summaries', color='blue')\n",
    "plt.plot(old_themes, old_percentages, label='Data of previous Summaries', color='red') #new percentages\n",
    "plt.xticks(rotation=45, ha='right') #Rotating and aligning the x-axis labels\n",
    "plt.xlabel('Comment Themes')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title(f'Most Common Comments on {date} and {old_date}')\n",
    "plt.legend()\n",
    "plt.tight_layout()  # Adjusts the layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas-AI experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandasai import SmartDataframe\n",
    "from pandasai.llm import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    api_token=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    api_version=\"2023-05-15\",\n",
    "    deployment_name=\"gpt-4-1106-preview\"\n",
    ")\n",
    "\n",
    "df = SmartDataframe(\"survey.csv\", config={'llm': llm}) #Loading the survey data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can ask specific question to the data\n",
    "For more granularity we can ask exact question about the data using PandasAI. Some Exmple queries are: -\n",
    "1. Give me the number of non-english speakers in the surver\n",
    "2. Give me a pie chart of the distribution of languages customers speak in the survey data with a clear legend\n",
    "3. How many customers were present in the survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = df.chat(\"How many people speak French in the Survey?\") #We can change the question here to the one we want to ask.\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
